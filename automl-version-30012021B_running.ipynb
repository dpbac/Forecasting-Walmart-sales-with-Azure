{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we use Azure AutoML to train, select, and operationalize a time-series forecasting model for time-series.\n",
    "\n",
    "Make sure you have executed the configuration notebook before running this notebook.\n",
    "\n",
    "The dataset used is available at . More details over the dataset are give in section [Dataset](#Dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "SDK version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "from train import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import joblib\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.automl.core.featurization import FeaturizationConfig\n",
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# onnx\n",
    "\n",
    "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
    "from azureml.automl.core.onnx_convert import OnnxConvertConstants\n",
    "from azureml.train.automl import constants\n",
    "import onnxruntime\n",
    "from azureml.automl.runtime.onnx_convert import OnnxInferenceHelper\n",
    "\n",
    "# Model deployment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check system and core SDK version number\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize workspace and create an Azure ML experiment\n",
    "\n",
    "To start we need to initialize our workspace and create a Azule ML experiment. It is also to remember that accessing the Azure ML workspace requires authentication with Azure.\n",
    "\n",
    "Make sure the config file is present at `.\\config.json`. This file can be downloaded from home of Azure Machine Learning Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick-starts-ws-136591\n",
      "aml-quickstarts-136591\n",
      "southcentralus\n",
      "5a4ab2ba-6c51-4805-8155-58759ad589d8\n"
     ]
    }
   ],
   "source": [
    "#Define the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>Report Page</th><th>Docs Page</th></tr><tr><td>automl-walmart-forecasting</td><td>quick-starts-ws-136591</td><td><a href=\"https://ml.azure.com/experiments/automl-walmart-forecasting?wsid=/subscriptions/5a4ab2ba-6c51-4805-8155-58759ad589d8/resourcegroups/aml-quickstarts-136591/workspaces/quick-starts-ws-136591\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.Experiment?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Experiment(Name: automl-walmart-forecasting,\n",
       "Workspace: quick-starts-ws-136591)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an experiment\n",
    "experiment_name = 'automl-walmart-forecasting'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Workspace name</th>\n",
       "      <td>quick-starts-ws-136591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Azure region</th>\n",
       "      <td>southcentralus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription id</th>\n",
       "      <td>5a4ab2ba-6c51-4805-8155-58759ad589d8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource group</th>\n",
       "      <td>aml-quickstarts-136591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment Name</th>\n",
       "      <td>automl-walmart-forecasting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     \n",
       "Workspace name                 quick-starts-ws-136591\n",
       "Azure region                           southcentralus\n",
       "Subscription id  5a4ab2ba-6c51-4805-8155-58759ad589d8\n",
       "Resource group                 aml-quickstarts-136591\n",
       "Experiment Name            automl-walmart-forecasting"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_data = {'Workspace name': ws.name,\n",
    "            'Azure region': ws.location,\n",
    "            'Subscription id': ws.subscription_id,\n",
    "            'Resource group': ws.resource_group,\n",
    "            'Experiment Name': experiment.name}\n",
    "\n",
    "df_data = pd.DataFrame.from_dict(data = dic_data, orient='index')\n",
    "\n",
    "df_data.rename(columns={0:''}, inplace = True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create or Attach an AmlCompute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Succeeded.......................\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 1, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-01-30T13:24:41.135000+00:00', 'errors': None, 'creationTime': '2021-01-30T13:22:32.959372+00:00', 'modifiedTime': '2021-01-30T13:22:48.307996+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_DS12_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Define CPU cluster name\n",
    "compute_target_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_target_name)\n",
    "    print(\"Found existing cpu-cluster. Use it.\")\n",
    "except ComputeTargetException:\n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_DS12_V2\",\n",
    "                                                           min_nodes=1, # when innactive\n",
    "                                                           max_nodes=4) # when busy\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    compute_target = ComputeTarget.create(ws, compute_target_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# For a more detailed view of current AmlCompute status, use get_status()\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset used in this project is a small subset of a much bigger dataset made available at Kaggle's competition [M5 Forecasting - Accuracy Estimate the unit sales of Walmart retail goods](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/description).\n",
    "\n",
    "The complete dataset covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. **The task is to forecast daily sales for the next 28 days.**\n",
    "\n",
    "In order to demonstrate the use of Azure ML in forecasting we used the available data consisting of the following files and create a reduced dataset with **10 products of the 3 Texas stores of Walmart**. \n",
    "\n",
    "* **calendar.csv** - Contains information about the dates on which the products are sold.\n",
    "* **sell_prices.csv** - Contains information about the price of the products sold per store and date.\n",
    "* **sales_train_evaluation.csv** - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n",
    "\n",
    "Details on how the new dataset was created can be seen in notebook [01-walmart_data_preparation](http://localhost:8888/notebooks/Capstone%20Project/notebooks/01-walmart_data_preparation.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workspace was initialized, an experiment was created as well as a compute target. Now is time to load the data into a pandas dataframe. \n",
    "\n",
    "The filtered data described previously was upload to GitHub. Therefore, we will use the GitHub link to upload the data. \n",
    "\n",
    "The time column is called `date`, so it will be parsed into the datetime type while loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_001_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_001</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_2_002_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_002</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_2_003_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_003</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_2_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_004</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_2_005_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_005</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_001_TX_1_evaluation  HOBBIES_2_001  HOBBIES_2  HOBBIES     TX_1   \n",
       "1  HOBBIES_2_002_TX_1_evaluation  HOBBIES_2_002  HOBBIES_2  HOBBIES     TX_1   \n",
       "2  HOBBIES_2_003_TX_1_evaluation  HOBBIES_2_003  HOBBIES_2  HOBBIES     TX_1   \n",
       "3  HOBBIES_2_004_TX_1_evaluation  HOBBIES_2_004  HOBBIES_2  HOBBIES     TX_1   \n",
       "4  HOBBIES_2_005_TX_1_evaluation  HOBBIES_2_005  HOBBIES_2  HOBBIES     TX_1   \n",
       "\n",
       "  state_id  demand       date  wm_yr_wk event_name_1 event_type_1  \\\n",
       "0       TX       0 2011-01-29     11101          NaN          NaN   \n",
       "1       TX       0 2011-01-29     11101          NaN          NaN   \n",
       "2       TX       0 2011-01-29     11101          NaN          NaN   \n",
       "3       TX       0 2011-01-29     11101          NaN          NaN   \n",
       "4       TX       0 2011-01-29     11101          NaN          NaN   \n",
       "\n",
       "  event_name_2 event_type_2  snap_TX  sell_price  \n",
       "0          NaN          NaN        0         nan  \n",
       "1          NaN          NaN        0        1.97  \n",
       "2          NaN          NaN        0         nan  \n",
       "3          NaN          NaN        0         nan  \n",
       "4          NaN          NaN        0         nan  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_column_name = 'date'\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/dpbac/Forecasting-Walmart-sales-with-Azure/master/data/walmart_tx_stores_10_items.csv?token=AEBB67P32BZUXMXJYVAQR63ACVPG6\", parse_dates=[time_column_name])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58230 entries, 0 to 58229\n",
      "Data columns (total 15 columns):\n",
      "id              58230 non-null object\n",
      "item_id         58230 non-null object\n",
      "dept_id         58230 non-null object\n",
      "cat_id          58230 non-null object\n",
      "store_id        58230 non-null object\n",
      "state_id        58230 non-null object\n",
      "demand          58230 non-null int64\n",
      "date            58230 non-null datetime64[ns]\n",
      "wm_yr_wk        58230 non-null int64\n",
      "event_name_1    4740 non-null object\n",
      "event_type_1    4740 non-null object\n",
      "event_name_2    120 non-null object\n",
      "event_type_2    120 non-null object\n",
      "snap_TX         58230 non-null int64\n",
      "sell_price      52938 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(3), object(10)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store TX_1 has 10 items\n",
      "Store TX_2 has 10 items\n",
      "Store TX_3 has 10 items\n"
     ]
    }
   ],
   "source": [
    "for store in data['store_id'].unique():\n",
    "    print(\"Store {} has {} items\".format(store,len(data['item_id'][data['store_id']=='TX_1'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build a time-series model for the `demand` column. As we saw this reduced dataset contains many individual time-series - one for each unique combination of `store_id` and `item_id`. Note that this is also translates by the unique items in column `ìd` which consists of concatenating `ìtem_id` with `store_id`.\n",
    "\n",
    "In order to distinguish the individual time-series, we define the `time_series_id_column_names` - the columns whose values determine the boundaries between time-series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 30 individual time-series.\n"
     ]
    }
   ],
   "source": [
    "time_series_id_column_names = ['store_id', 'item_id']\n",
    "nseries = data.groupby(time_series_id_column_names).ngroups\n",
    "print('Data contains {0} individual time-series.'.format(nseries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we confirm what was said checking the unique item in column id\n",
    "len(data['id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Before spliting data we replace NaN in events information by `\"no_event\"`.\n",
    "\n",
    "In the description of the dataset given in the [Guideline](https://mofc.unic.ac.cy/wp-content/uploads/2020/03/M5-Competitors-Guide-Final-10-March-2020.docx) says: \"The description of the dataset says: \"If the date includes an event, the name of this event.\". Therefore, I'll replace NaN values by `\"no_event\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = replace_nan_events(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Time Series Dataset\n",
    "\n",
    "We now split the data into a training and a testing datasets. The test set will contain the final 28 days of observed demand for each time-series. This will be used later forecast evaluation.\n",
    "\n",
    "The function `split_train_test()` splits our time series data into training and testing following the rule that test indices need to be higher than the train indices, and higher than the test indices in the previous split.\n",
    "\n",
    "The test split is `forecast_horizon` days long, and the testing period is `gap` number of days away from the training period. In practice, this gap is used to allow business managers to plan for the forecasted demand.\n",
    "\n",
    "The first available day in the data (or the first day we want to start modeling from) is **2011-01-29**, and the last day is **2016-05-22**.\n",
    "\n",
    "The image below illustrates how the data will be split by `split_train_test()`.\n",
    "\n",
    "Consider, that our current day is **2016-04-24** and that we want to forecast 28 days without gap, i.e., we want to forecast demand from **2016-04-25** until **2016-05-22**. Therefore, in this case the parameters used will be:\n",
    "\n",
    "* `forecast_horizon`=28 \n",
    "* `gap`= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/time_series_split.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/dpbac/Forecasting-Walmart-sales-with-Azure/blob/master/images/time_series_split.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day training dataset:2011-01-29 00:00:00\n",
      "Last day training dataset:2016-04-24 00:00:00\n",
      "First day test dataset:2016-04-25 00:00:00\n",
      "Last day test dataset:2016-05-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = data\n",
    "forecast_horizon = 28\n",
    "gap = 0\n",
    "\n",
    "df_train, df_test = split_train_test(df,forecast_horizon, gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to datastore\n",
    "\n",
    "**REPLACE AND EDIT FOLLOWING TEXT**\n",
    "\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace), is paired with the storage account, which contains the default data store. We will use it to upload the train and test data and create [tabular datasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training and testing. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading ./train.csv\n",
      "Uploaded ./train.csv, 1 files out of an estimated total of 2\n",
      "Uploading ./test.csv\n",
      "Uploaded ./test.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_a90baaa707af433bbb22bbce7a29f3fa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save data locally\n",
    "    \n",
    "path_train = './train.csv'\n",
    "path_test = './test.csv'\n",
    "df_train.to_csv(path_train, index = None, header=True)\n",
    "df_test.to_csv(path_test, index = None, header=True)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./train.csv', './test.csv'], \n",
    "                       target_path = 'dataset/', \n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataset\n",
    "\n",
    "df = train_dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_001_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_001</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_2_002_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_002</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_2_003_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_003</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_2_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_004</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_2_005_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_005</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_001_TX_1_evaluation  HOBBIES_2_001  HOBBIES_2  HOBBIES     TX_1   \n",
       "1  HOBBIES_2_002_TX_1_evaluation  HOBBIES_2_002  HOBBIES_2  HOBBIES     TX_1   \n",
       "2  HOBBIES_2_003_TX_1_evaluation  HOBBIES_2_003  HOBBIES_2  HOBBIES     TX_1   \n",
       "3  HOBBIES_2_004_TX_1_evaluation  HOBBIES_2_004  HOBBIES_2  HOBBIES     TX_1   \n",
       "4  HOBBIES_2_005_TX_1_evaluation  HOBBIES_2_005  HOBBIES_2  HOBBIES     TX_1   \n",
       "\n",
       "  state_id  demand       date  wm_yr_wk event_name_1 event_type_1  \\\n",
       "0       TX       0 2011-01-29     11101     no_event     no_event   \n",
       "1       TX       0 2011-01-29     11101     no_event     no_event   \n",
       "2       TX       0 2011-01-29     11101     no_event     no_event   \n",
       "3       TX       0 2011-01-29     11101     no_event     no_event   \n",
       "4       TX       0 2011-01-29     11101     no_event     no_event   \n",
       "\n",
       "  event_name_2 event_type_2  snap_TX  sell_price  \n",
       "0     no_event     no_event        0         nan  \n",
       "1     no_event     no_event        0        1.97  \n",
       "2     no_event     no_event        0         nan  \n",
       "3     no_event     no_event        0         nan  \n",
       "4     no_event     no_event        0         nan  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57385</th>\n",
       "      <td>HOBBIES_2_006_TX_3_evaluation</td>\n",
       "      <td>HOBBIES_2_006</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_3</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57386</th>\n",
       "      <td>HOBBIES_2_007_TX_3_evaluation</td>\n",
       "      <td>HOBBIES_2_007</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_3</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57387</th>\n",
       "      <td>HOBBIES_2_008_TX_3_evaluation</td>\n",
       "      <td>HOBBIES_2_008</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_3</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57388</th>\n",
       "      <td>HOBBIES_2_009_TX_3_evaluation</td>\n",
       "      <td>HOBBIES_2_009</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_3</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>5.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57389</th>\n",
       "      <td>HOBBIES_2_010_TX_3_evaluation</td>\n",
       "      <td>HOBBIES_2_010</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_3</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        item_id    dept_id   cat_id  \\\n",
       "57385  HOBBIES_2_006_TX_3_evaluation  HOBBIES_2_006  HOBBIES_2  HOBBIES   \n",
       "57386  HOBBIES_2_007_TX_3_evaluation  HOBBIES_2_007  HOBBIES_2  HOBBIES   \n",
       "57387  HOBBIES_2_008_TX_3_evaluation  HOBBIES_2_008  HOBBIES_2  HOBBIES   \n",
       "57388  HOBBIES_2_009_TX_3_evaluation  HOBBIES_2_009  HOBBIES_2  HOBBIES   \n",
       "57389  HOBBIES_2_010_TX_3_evaluation  HOBBIES_2_010  HOBBIES_2  HOBBIES   \n",
       "\n",
       "      store_id state_id  demand       date  wm_yr_wk event_name_1  \\\n",
       "57385     TX_3       TX       0 2016-04-24     11613     no_event   \n",
       "57386     TX_3       TX       0 2016-04-24     11613     no_event   \n",
       "57387     TX_3       TX       0 2016-04-24     11613     no_event   \n",
       "57388     TX_3       TX       0 2016-04-24     11613     no_event   \n",
       "57389     TX_3       TX       0 2016-04-24     11613     no_event   \n",
       "\n",
       "      event_type_1 event_name_2 event_type_2  snap_TX  sell_price  \n",
       "57385     no_event     no_event     no_event        0        3.97  \n",
       "57386     no_event     no_event     no_event        0        0.97  \n",
       "57387     no_event     no_event     no_event        0        3.88  \n",
       "57388     no_event     no_event     no_event        0        5.97  \n",
       "57389     no_event     no_event     no_event        0        0.97  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57390 entries, 0 to 57389\n",
      "Data columns (total 15 columns):\n",
      "id              57390 non-null object\n",
      "item_id         57390 non-null object\n",
      "dept_id         57390 non-null object\n",
      "cat_id          57390 non-null object\n",
      "store_id        57390 non-null object\n",
      "state_id        57390 non-null object\n",
      "demand          57390 non-null int64\n",
      "date            57390 non-null datetime64[ns]\n",
      "wm_yr_wk        57390 non-null int64\n",
      "event_name_1    57390 non-null object\n",
      "event_type_1    57390 non-null object\n",
      "event_name_2    57390 non-null object\n",
      "event_type_2    57390 non-null object\n",
      "snap_TX         57390 non-null int64\n",
      "sell_price      52098 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(3), object(10)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Modeling\n",
    "\n",
    "**REVIEW AND EDIT**\n",
    "\n",
    "For forecasting tasks, AutoML uses pre-processing and estimation steps that are specific to time-series. AutoML will undertake the following pre-processing steps:\n",
    "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
    "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
    "* Create features based on time series identifiers to enable fixed effects across different series\n",
    "* Create time-based features to assist in learning seasonal patterns\n",
    "* Encode categorical variables to numeric quantities\n",
    "\n",
    "In this notebook, AutoML will train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series. If you're looking for training multiple models for different time-series, please see the many-models notebook.\n",
    "\n",
    "You are almost ready to start an AutoML training job. First, we need to separate the target column from the rest of the DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'demand'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IF NECESSARY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "**REVIEW, EDIT, MINIMIZE**\n",
    "\n",
    "The featurization customization in forecasting is an advanced feature in AutoML which allows our customers to change the default forecasting featurization behaviors and column types through `FeaturizationConfig`. The supported scenarios include:\n",
    "\n",
    "1. Column purposes update: Override feature type for the specified column. Currently supports DateTime, Categorical and Numeric. This customization can be used in the scenario that the type of the column cannot correctly reflect its purpose. Some numerical columns, for instance, can be treated as Categorical columns which need to be converted to categorical while some can be treated as epoch timestamp which need to be converted to datetime. To tell our SDK to correctly preprocess these columns, a configuration need to be add with the columns and their desired types.\n",
    "2. Transformer parameters update: Currently supports parameter change for Imputer only. User can customize imputation methods. The supported imputing methods for target column are constant and ffill (forward fill). The supported imputing methods for feature columns are mean, median, most frequent, constant and ffill (forward fill). This customization can be used for the scenario that our customers know which imputation methods fit best to the input data. For instance, some datasets use NaN to represent 0 which the correct behavior should impute all the missing value with 0. To achieve this behavior, these columns need to be configured as constant imputation with `fill_value` 0.\n",
    "3. Drop columns: Columns to drop from being featurized. These usually are the columns which are leaky or the columns contain no useful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurization_config = FeaturizationConfig()\n",
    "# featurization_config.drop_columns = ['logQuantity']  # 'logQuantity' is a leaky feature, so we remove it.\n",
    "# # Force the CPWVOL5 feature to be numeric type.\n",
    "# featurization_config.add_column_purpose('CPWVOL5', 'Numeric')\n",
    "# # Fill missing values in the target column, Quantity, with zeros.\n",
    "# featurization_config.add_transformer_params('Imputer', ['Quantity'], {\"strategy\": \"constant\", \"fill_value\": 0})\n",
    "# # Fill missing values in the INCOME column with median value.\n",
    "# featurization_config.add_transformer_params('Imputer', ['INCOME'], {\"strategy\": \"median\"})\n",
    "# # Fill missing values in the Price column with forward fill (last value carried forward).\n",
    "# featurization_config.add_transformer_params('Imputer', ['Price'], {\"strategy\": \"ffill\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAYBE - TRY WITHOUT\n",
    "\n",
    "# featurization_config = FeaturizationConfig()\n",
    "# Fill missing values in the Price column with forward fill (last value carried forward).\n",
    "# featurization_config.add_transformer_params('Imputer', ['sell_price'], {\"strategy\": \"ffill\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Parameters\n",
    "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
    "\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**time_series_id_column_names**|The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, the data set is assumed to be one time series.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = 'date'\n",
    "forecast_horizon = 28\n",
    "time_series_id_column_names = ['item_id','store_id'] # or ['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration - Train\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below.\n",
    "\n",
    "**REVIEW AND EDIT**\n",
    "\n",
    "The [AutoMLConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py) object defines the settings and data for an AutoML training job. Here, we set necessary inputs like the task type, the number of AutoML iterations to try, the training data, and cross-validation parameters.\n",
    "\n",
    "For forecasting tasks, there are some additional parameters that can be set in the `ForecastingParameters` class: the name of the column holding the date/time, the timeseries id column names, and the maximum forecast horizon. A time column is required for forecasting, while the time_series_id is optional. If time_series_id columns are not given, AutoML assumes that the whole dataset is a single time-series. We also pass a list of columns to drop prior to modeling. The _logQuantity_ column is completely correlated with the target quantity, so it must be removed to prevent a target leak.\n",
    "\n",
    "The forecast horizon is given in units of the time-series frequency; for instance, the OJ series frequency is weekly, so a horizon of 20 means that a trained model will estimate sales up to 20 weeks beyond the latest date in the training data for each series. In this example, we set the forecast horizon to the number of samples per series in the test set (n_test_periods). Generally, the value of this parameter will be dictated by business needs. For example, a demand planning application that estimates the next month of sales should set the horizon according to suitable planning time-scales. Please see the [energy_demand notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand) for more discussion of forecast horizon.\n",
    "\n",
    "We note here that AutoML can sweep over two types of time-series models:\n",
    "* Models that are trained for each series such as ARIMA and Facebook's Prophet.\n",
    "* Models trained across multiple time-series using a regression approach.\n",
    "\n",
    "In the first case, AutoML loops over all time-series in your dataset and trains one model (e.g. AutoArima or Prophet, as the case may be) for each series. This can result in long runtimes to train these models if there are a lot of series in the data. One way to mitigate this problem is to fit models for different series in parallel if you have multiple compute cores available. To enable this behavior, set the `max_cores_per_iteration` parameter in your AutoMLConfig as shown in the example in the next cell. \n",
    "\n",
    "\n",
    "Finally, a note about the cross-validation (CV) procedure for time-series data. AutoML uses out-of-sample error estimates to select a best pipeline/model, so it is important that the CV fold splitting is done correctly. Time-series can violate the basic statistical assumptions of the canonical K-Fold CV strategy, so AutoML implements a [rolling origin validation](https://robjhyndman.com/hyndsight/tscv/) procedure to create CV folds for time-series data. To use this procedure, you just need to specify the desired number of CV folds in the AutoMLConfig object. It is also possible to bypass CV and use your own validation set by setting the *validation_data* parameter of AutoMLConfig.\n",
    "\n",
    "Here is a summary of AutoMLConfig parameters used for training the OJ model:\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**experiment_timeout_hours**|Experimentation timeout in hours.|\n",
    "|**enable_early_stopping**|If early stopping is on, training will stop when the primary metric is no longer improving.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**compute_target**|The remote compute for training.|\n",
    "|**n_cross_validations**|Number of cross-validation folds to use for model/pipeline selection|\n",
    "|**enable_voting_ensemble**|Allow AutoML to create a Voting ensemble of the best performing models|\n",
    "|**enable_stack_ensemble**|Allow AutoML to create a Stack ensemble of the best performing models|\n",
    "|**debug_log**|Log file path for writing debugging information|\n",
    "|**featurization**| 'auto' / 'off' / FeaturizationConfig Indicator for whether featurization step should be done automatically or not, or whether customized featurization should be used. Setting this enables AutoML to perform featurization on the input to handle *missing data*, and to perform some common *feature extraction*.|\n",
    "|**max_cores_per_iteration**|Maximum number of cores to utilize per iteration. A value of -1 indicates all available cores should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    time_series_id_column_names=time_series_id_column_names\n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl_walmart_forecasting_errors.log',\n",
    "                             primary_metric='normalized_mean_absolute_error',\n",
    "                             experiment_timeout_minutes=30,\n",
    "                             training_data=train_dataset,\n",
    "                             label_column_name=target_column_name,\n",
    "                             compute_target=compute_target,\n",
    "#                              enable_onnx_compatible_models=True,\n",
    "                             enable_early_stopping=True,\n",
    "#                              featurization=featurization_config, #try using and not using\n",
    "                             n_cross_validations=3,\n",
    "                             verbosity=logging.INFO,\n",
    "                             max_cores_per_iteration=-1,\n",
    "                             forecasting_parameters=forecasting_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote.\n",
      "No run_configuration provided, running on cpu-cluster with default configuration\n",
      "Running on remote compute: cpu-cluster\n",
      "Parent Run ID: AutoML_8b1377ab-5ae4-4133-842d-3651879c93f7\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the CV split.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Frequency detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  The time series was analyzed, all data points are aligned with detected frequency.\n",
      "              \n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "TYPE:         Missing feature values imputation\n",
      "STATUS:       DONE\n",
      "DESCRIPTION:  If the missing values are expected, let the run complete. Otherwise cancel the current run and use a script to customize the handling of missing feature values that may be more appropriate based on the data type and business requirement.\n",
      "              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n",
      "DETAILS:      \n",
      "+---------------------------------+---------------------------------+---------------------------------+\n",
      "|Column name                      |Missing value count              |Imputation type                  |\n",
      "+=================================+=================================+=================================+\n",
      "|sell_price                       |5292                             |median                           |\n",
      "+---------------------------------+---------------------------------+---------------------------------+\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "TYPE:         Short series handling\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Automated ML detected enough data points for each series in the input data to continue with training.\n",
      "              \n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0    AutoArima                                     0:02:26       0.0794    0.0794\n",
      "         1    ProphetModel                                  0:10:30       0.0777    0.0777\n",
      "         2   RobustScaler DecisionTree                      0:00:55       0.0801    0.0777\n",
      "         3   StandardScalerWrapper DecisionTree             0:00:58       0.0781    0.0777\n",
      "         4   MinMaxScaler LassoLars                         0:01:50       0.0864    0.0777\n",
      "         5   StandardScalerWrapper LassoLars                0:00:47       0.0863    0.0777\n",
      "         6   MinMaxScaler DecisionTree                      0:00:51       0.0793    0.0777\n",
      "         7   RobustScaler DecisionTree                      0:00:48       0.0802    0.0777\n",
      "         8   StandardScalerWrapper ElasticNet               0:00:49       0.0857    0.0777\n",
      "         9   StandardScalerWrapper DecisionTree             0:00:49       0.0784    0.0777\n",
      "        10   RobustScaler DecisionTree                      0:00:52       0.0791    0.0777\n",
      "        11   MinMaxScaler DecisionTree                      0:00:50       0.0800    0.0777\n",
      "        12   StandardScalerWrapper DecisionTree             0:00:58       0.0786    0.0777\n"
     ]
    }
   ],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=True)\n",
    "remote_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Launch the widget to view the progress and results\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Save the Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving the best model\n",
    "best_run, best_model = remote_run.get_output()\n",
    "print(best_model.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_metrics = best_run.get_metrics()\n",
    "print(\"Best run Id: \",best_run.id)\n",
    "print(\"NMAE: \", best_run_metrics['normalized_mean_absolute_error'])\n",
    "print(\"Other details: \")\n",
    "print(\"Fitted model:\",best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best model\n",
    "os.makedirs('results', exist_ok=True)\n",
    "automl_model_name = best_run.properties['model_name']\n",
    "\n",
    "joblib.dump(best_model, filename=\"results/automl_forecasting_model.pkl\")\n",
    "model = remote_run.register_model(model_name=automl_model_name, description='Best AutoML Walmart forecasting model')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data. First, we remove the target values from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test\n",
    "y_test = X_test.pop(target_column_name).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REVIEW AND EDIT**\n",
    "\n",
    "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast returns the predictions and the featurized data, aligned to X_test.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "y_predictions, X_trans = best_model.forecast(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "**REVIEW AND EDIT**\n",
    "\n",
    "To evaluate the accuracy of the forecast, we'll compare against the actual sales quantities for some select metrics, included the mean absolute percentage error (MAPE). \n",
    "\n",
    "We'll add predictions and actuals into a single dataframe for convenience in calculating the metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_dict = {'predicted': y_predictions, target_column_name: y_test}\n",
    "df_all = X_test.assign(**assign_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.automl.runtime.shared.score import scoring\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# use automl scoring module\n",
    "scores = scoring.score_regression(\n",
    "    y_test=df_all[target_column_name],\n",
    "    y_pred=df_all['predicted'],\n",
    "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
    "\n",
    "print(\"[Test data scores]\\n\")\n",
    "for key, value in scores.items():    \n",
    "    print('{}:   {:.3f}'.format(key, value))\n",
    "    \n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service.\n",
    "\n",
    "**REVIEW AND EDIT**\n",
    "\n",
    "_Operationalization_ (or Deployment) means getting the model into the cloud so that other can run it after you close the notebook. We will create a docker running on Azure Container Instances with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\"Model name: {}\".format(automl_model_name))\n",
    "print(remote_run.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get function which will run the forecast on serialized data from the best_run\n",
    "script_file_name = 'score_forecast.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', script_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model as a Web Service on Azure Container Instance (ACI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(environment = best_run.get_environment(), \n",
    "                                   entry_script = script_file_name)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 2, \n",
    "                                               auth_enabled=True, \n",
    "                                               enable_app_insights=True,\n",
    "                                               tags = {'type': \"automl-forecasting\"},\n",
    "                                               description = \"Automl forecasting sample service\")\n",
    "\n",
    "aci_service_name = 'automl-walmart-forecast-01'\n",
    "print(aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Gets logs from a deployed web service.\n",
    "aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment state: \" + aci_service.state)\n",
    "print(\"Scoring URI: \" + aci_service.scoring_uri)\n",
    "print(\"Authetication Key: \" + aci_service.get_keys()[0])\n",
    "print(\"Swagger URI: \" + aci_service.swagger_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe erase the following (endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "X_query = X_test.copy()\n",
    "\n",
    "# We have to convert datetime to string, because Timestamps cannot be serialized to JSON.\n",
    "X_query[time_column_name] = X_query[time_column_name].astype(str)\n",
    "\n",
    "# The Service object accept the complex dictionary, which is internally converted to JSON string.\n",
    "# The section 'data' contains the data frame in the form of dictionary.\n",
    "test_sample = json.dumps({'data': X_query.to_dict(orient='records')})\n",
    "response = aci_service.run(input_data = test_sample)\n",
    "# translate from networkese to datascientese\n",
    "try: \n",
    "    res_dict = json.loads(response)\n",
    "    y_forecast_all = pd.DataFrame(res_dict['index'])\n",
    "    y_forecast_all[time_column_name] = pd.to_datetime(y_forecast_all[time_column_name], unit = 'ms')\n",
    "    y_forecast_all['forecast'] = res_dict['forecast']    \n",
    "except:\n",
    "    print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX model\n",
    "\n",
    "## Retrieve and save the best ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OnnxConvertException",
     "evalue": "OnnxConvertException:\n\tMessage: Requested an ONNX compatible model but the run has ONNX compatibility disabled.\n\tInnerException: None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Requested an ONNX compatible model but the run has ONNX compatibility disabled.\",\n        \"target\": \"onnx_compatible\",\n        \"inner_error\": {\n            \"code\": \"BadArgument\",\n            \"inner_error\": {\n                \"code\": \"ArgumentInvalid\"\n            }\n        }\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOnnxConvertException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5301998e8f01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Retrieve and save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremote_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_onnx_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0monnx_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/best_model.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOnnxConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_onnx_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/train/automl/run.py\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(self, iteration, metric, return_onnx_model, return_split_onnx_model, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0monnx_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                     \u001b[0msplit_onnx_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_split_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                     \u001b[0mactivity_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoml_shared_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTelemetryConstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_OUTPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m                 )\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/train/automl/run.py\u001b[0m in \u001b[0;36m_get_best_child_run\u001b[0;34m(self, iteration, metric, onnx_compatible, split_onnx_model_name, activity_name)\u001b[0m\n\u001b[1;32m    601\u001b[0m                         raise OnnxConvertException._with_error(\n\u001b[1;32m    602\u001b[0m                             AzureMLError.create(\n\u001b[0;32m--> 603\u001b[0;31m                                 OnnxNotEnabled, target=\"onnx_compatible\")\n\u001b[0m\u001b[1;32m    604\u001b[0m                         )\n\u001b[1;32m    605\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msplit_onnx_model_name\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mautoml_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_split_onnx_featurizer_estimator_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOnnxConvertException\u001b[0m: OnnxConvertException:\n\tMessage: Requested an ONNX compatible model but the run has ONNX compatibility disabled.\n\tInnerException: None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Requested an ONNX compatible model but the run has ONNX compatibility disabled.\",\n        \"target\": \"onnx_compatible\",\n        \"inner_error\": {\n            \"code\": \"BadArgument\",\n            \"inner_error\": {\n                \"code\": \"ArgumentInvalid\"\n            }\n        }\n    }\n}"
     ]
    }
   ],
   "source": [
    "#Retrieve and save the best model\n",
    "\n",
    "best_run, onnx_model = remote_run.get_output(return_onnx_model=True)\n",
    "onnx_model_path = \"results/best_model.onnx\"\n",
    "OnnxConverter.save_onnx_model(onnx_model, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-54d536d02b05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpython_version_compatible\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmdl_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0monnx_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_onnx_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "if sys.version_info < OnnxConvertConstants.OnnxIncompatiblePythonVersion:\n",
    "    python_version_compatible = True\n",
    "else:\n",
    "    python_version_compatible = False\n",
    "\n",
    "def get_onnx_res(run):\n",
    "    res_path = 'onnx_resource.json'\n",
    "    run.download_file(name=constants.MODEL_RESOURCE_PATH_ONNX, output_file_path=res_path)\n",
    "    with open(res_path) as f:\n",
    "        onnx_res = json.load(f)\n",
    "    return onnx_res\n",
    "\n",
    "if python_version_compatible:\n",
    "    test_df = test_data.to_pandas_dataframe()\n",
    "    mdl_bytes = onnx_mdl.SerializeToString()\n",
    "    onnx_res = get_onnx_res(best_run)\n",
    "\n",
    "    onnxrt_helper = OnnxInferenceHelper(mdl_bytes, onnx_res)\n",
    "    pred_onnx, pred_prob_onnx = onnxrt_helper.predict(test_df)\n",
    "\n",
    "    print(pred_onnx)\n",
    "    print(pred_prob_onnx)\n",
    "else:\n",
    "    print('Use Python version 3.6 or 3.7 to run the inference helper.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AciWebservice(workspace=Workspace.create(name='quick-starts-ws-136591', subscription_id='5a4ab2ba-6c51-4805-8155-58759ad589d8', resource_group='aml-quickstarts-136591'), name=automl-walmart-forecast-01, image_id=None, compute_type=None, state=ACI, scoring_uri=Healthy, tags=http://da310598-54a4-4127-8836-462a8193e64a.southcentralus.azurecontainer.io/score, properties={'type': 'automl-forecasting'}, created_by={'hasInferenceSchema': 'True', 'hasHttps': 'False'})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serv = Webservice(ws, 'automl-walmart-forecast-01')\n",
    "serv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "serv.delete()     # don't do it accidentally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
