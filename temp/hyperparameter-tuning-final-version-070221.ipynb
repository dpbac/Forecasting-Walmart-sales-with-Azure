{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "In this notebook, we use `HyperDrive` to tune hyperparameters, train, select, and operationalize a time-series forecasting model that forecasts daily sales for the next 28 days of Walmart hobbies products in Texas.\n",
    "\n",
    "The algorithm used by HyperDrive is Light GBM known for being a Kaggle's competition winner.\n",
    "\n",
    "The dataset used is a subset of the one made available from [Kaggle's competition M5 Forecasting - Accuracy ](https://www.kaggle.com/c/m5-forecasting-accuracy/data) is available at [GitHub](https://github.com/dpbac/Forecasting-Walmart-sales-with-Azure/blob/master/data/walmart_tx_stores_10_items_with_day.csv). \n",
    "\n",
    "More details over the dataset are give in section [Dataset](#Dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "Azure ML SDK version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import azureml\n",
    "import requests \n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.sampling import BayesianParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, quniform, choice\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import EnvironmentDefinition\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "from azureml.train.automl import constants\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from train import *\n",
    "\n",
    "# Check system and core SDK version number\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize workspace and create an Azure ML experiment\n",
    "\n",
    "To start we need to initialize our workspace and create a Azule ML experiment. It is also to remember that accessing the Azure ML workspace requires authentication with Azure.\n",
    "\n",
    "Make sure the config file is present at `.\\config.json`. This file can be downloaded from home of Azure Machine Learning Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick-starts-ws-137902\n",
      "aml-quickstarts-137902\n",
      "southcentralus\n",
      "aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\n"
     ]
    }
   ],
   "source": [
    "#Define the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>Report Page</th><th>Docs Page</th></tr><tr><td>hyper-lgbm-walmart-forecasting</td><td>quick-starts-ws-137902</td><td><a href=\"https://ml.azure.com/experiments/hyper-lgbm-walmart-forecasting?wsid=/subscriptions/aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee/resourcegroups/aml-quickstarts-137902/workspaces/quick-starts-ws-137902\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.Experiment?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Experiment(Name: hyper-lgbm-walmart-forecasting,\n",
       "Workspace: quick-starts-ws-137902)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an experiment\n",
    "experiment_name = 'hyper-lgbm-walmart-forecasting'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Workspace name</th>\n",
       "      <td>quick-starts-ws-137902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Azure region</th>\n",
       "      <td>southcentralus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription id</th>\n",
       "      <td>aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource group</th>\n",
       "      <td>aml-quickstarts-137902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment Name</th>\n",
       "      <td>hyper-lgbm-walmart-forecasting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     \n",
       "Workspace name                 quick-starts-ws-137902\n",
       "Azure region                           southcentralus\n",
       "Subscription id  aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\n",
       "Resource group                 aml-quickstarts-137902\n",
       "Experiment Name        hyper-lgbm-walmart-forecasting"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_data = {'Workspace name': ws.name,\n",
    "            'Azure region': ws.location,\n",
    "            'Subscription id': ws.subscription_id,\n",
    "            'Resource group': ws.resource_group,\n",
    "            'Experiment Name': experiment.name}\n",
    "\n",
    "df_data = pd.DataFrame.from_dict(data = dic_data, orient='index')\n",
    "\n",
    "df_data.rename(columns={0:''}, inplace = True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create or Attach an AmlCompute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cpu-cluster. Use it.\n",
      "\n",
      "Running\n",
      "{'errors': [], 'creationTime': '2021-02-07T19:10:34.656218+00:00', 'createdBy': {'userObjectId': '13845e32-939c-40b2-b2ab-8410d61dca96', 'userTenantId': '660b3398-b80e-49d2-bc5b-ac1dc93b5254', 'userName': None}, 'modifiedTime': '2021-02-07T19:13:36.133076+00:00', 'state': 'Running', 'vmSize': 'STANDARD_DS12_V2'}\n"
     ]
    }
   ],
   "source": [
    "# Define CPU cluster name\n",
    "compute_target_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_target_name)\n",
    "    print(\"Found existing cpu-cluster. Use it.\")\n",
    "except ComputeTargetException:\n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_DS12_V2\",\n",
    "                                                           min_nodes=1, # when innactive\n",
    "                                                           max_nodes=4) # when busy\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    compute_target = ComputeTarget.create(ws, compute_target_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# For a more detailed view of current AmlCompute status, use get_status()\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Docker environment\n",
    "\n",
    "The remote compute will need to create a [Docker image](https://docs.docker.com/get-started/) for running the script. The Docker image is an encapsulated environment with necessary dependencies installed. In the following cell, we specify the conda packages and Python version that are needed for running the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvironmentDefinition()\n",
    "env.python.user_managed_dependencies = False\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=[\"pandas\", \"numpy\", \"scipy\", \"scikit-learn\", \"lightgbm\", \"joblib\"],\n",
    "    python_version=\"3.6.2\",\n",
    ")\n",
    "env.python.conda_dependencies.add_channel(\"conda-forge\")\n",
    "env.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset used in this project is a small subset of a much bigger dataset made available at Kaggle's competition [M5 Forecasting - Accuracy Estimate the unit sales of Walmart retail goods](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/description).\n",
    "\n",
    "The complete dataset covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. **The task is to forecast daily sales for the next 28 days.**\n",
    "\n",
    "In order to demonstrate the use of Azure ML in forecasting we used the available data consisting of the following files and create a reduced dataset with **10 products of the 3 Texas stores of Walmart**. \n",
    "\n",
    "* **calendar.csv** - Contains information about the dates on which the products are sold.\n",
    "* **sell_prices.csv** - Contains information about the price of the products sold per store and date.\n",
    "* **sales_train_evaluation.csv** - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n",
    "\n",
    "Details on how the new dataset was created can be seen in notebook [01-walmart_data_preparation](http://localhost:8888/notebooks/Capstone%20Project/notebooks/01-walmart_data_preparation.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_001_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_001</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_2_002_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_002</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_2_003_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_003</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_2_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_004</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_2_005_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_005</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_001_TX_1_evaluation  HOBBIES_2_001  HOBBIES_2  HOBBIES     TX_1   \n",
       "1  HOBBIES_2_002_TX_1_evaluation  HOBBIES_2_002  HOBBIES_2  HOBBIES     TX_1   \n",
       "2  HOBBIES_2_003_TX_1_evaluation  HOBBIES_2_003  HOBBIES_2  HOBBIES     TX_1   \n",
       "3  HOBBIES_2_004_TX_1_evaluation  HOBBIES_2_004  HOBBIES_2  HOBBIES     TX_1   \n",
       "4  HOBBIES_2_005_TX_1_evaluation  HOBBIES_2_005  HOBBIES_2  HOBBIES     TX_1   \n",
       "\n",
       "  state_id  day  demand       date  wm_yr_wk event_name_1 event_type_1  \\\n",
       "0       TX  d_1       0 2011-01-29     11101          NaN          NaN   \n",
       "1       TX  d_1       0 2011-01-29     11101          NaN          NaN   \n",
       "2       TX  d_1       0 2011-01-29     11101          NaN          NaN   \n",
       "3       TX  d_1       0 2011-01-29     11101          NaN          NaN   \n",
       "4       TX  d_1       0 2011-01-29     11101          NaN          NaN   \n",
       "\n",
       "  event_name_2 event_type_2  snap_TX  sell_price  \n",
       "0          NaN          NaN        0         NaN  \n",
       "1          NaN          NaN        0        1.97  \n",
       "2          NaN          NaN        0         NaN  \n",
       "3          NaN          NaN        0         NaN  \n",
       "4          NaN          NaN        0         NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_column_name = 'date'\n",
    "# data = pd.read_csv(\"./data/walmart_tx_stores_10_items_with_day.csv\",parse_dates=[time_column_name])\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/dpbac/Forecasting-Walmart-sales-with-Azure/master/data/walmart_tx_stores_10_items_with_day.csv\", parse_dates=[time_column_name])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58230 entries, 0 to 58229\n",
      "Data columns (total 16 columns):\n",
      "id              58230 non-null object\n",
      "item_id         58230 non-null object\n",
      "dept_id         58230 non-null object\n",
      "cat_id          58230 non-null object\n",
      "store_id        58230 non-null object\n",
      "state_id        58230 non-null object\n",
      "day             58230 non-null object\n",
      "demand          58230 non-null int64\n",
      "date            58230 non-null datetime64[ns]\n",
      "wm_yr_wk        58230 non-null int64\n",
      "event_name_1    4740 non-null object\n",
      "event_type_1    4740 non-null object\n",
      "event_name_2    120 non-null object\n",
      "event_type_2    120 non-null object\n",
      "snap_TX         58230 non-null int64\n",
      "sell_price      52938 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(3), object(11)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 28\n",
    "gap = 0\n",
    "\n",
    "data = create_features(data,forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41988 entries, 10951 to 58229\n",
      "Data columns (total 42 columns):\n",
      "id                          41988 non-null category\n",
      "item_id                     41988 non-null category\n",
      "dept_id                     41988 non-null category\n",
      "cat_id                      41988 non-null category\n",
      "store_id                    41988 non-null category\n",
      "state_id                    41988 non-null category\n",
      "day                         41988 non-null category\n",
      "demand                      41988 non-null int64\n",
      "date                        41988 non-null datetime64[ns]\n",
      "wm_yr_wk                    41988 non-null int64\n",
      "event_name_1                41988 non-null category\n",
      "event_type_1                41988 non-null category\n",
      "event_name_2                41988 non-null category\n",
      "event_type_2                41988 non-null category\n",
      "snap_TX                     41988 non-null int64\n",
      "sell_price                  41988 non-null float64\n",
      "lag_t28                     41988 non-null float64\n",
      "lag_t29                     41988 non-null float64\n",
      "lag_t30                     41988 non-null float64\n",
      "rolling_mean_t7             41988 non-null float64\n",
      "rolling_std_t7              41988 non-null float64\n",
      "rolling_mean_t30            41988 non-null float64\n",
      "rolling_std_t30             41988 non-null float64\n",
      "rolling_mean_t90            41988 non-null float64\n",
      "rolling_std_t90             41988 non-null float64\n",
      "rolling_mean_t180           41988 non-null float64\n",
      "rolling_std_t180            41988 non-null float64\n",
      "price_change_t1             41988 non-null float64\n",
      "price_change_t365           41988 non-null float64\n",
      "rolling_price_std_t7        41988 non-null float64\n",
      "rolling_price_std_t30       41988 non-null float64\n",
      "day_of_month                41988 non-null int64\n",
      "day_of_week                 41988 non-null int64\n",
      "week                        41988 non-null int64\n",
      "month                       41988 non-null int64\n",
      "year                        41988 non-null int64\n",
      "is_month_start              41988 non-null int64\n",
      "is_month_end                41988 non-null int64\n",
      "is_weekend                  41988 non-null int64\n",
      "lag_revenue_t1              41988 non-null float64\n",
      "rolling_revenue_std_t28     41988 non-null float64\n",
      "rolling_revenue_mean_t28    41988 non-null float64\n",
      "dtypes: category(11), datetime64[ns](1), float64(19), int64(11)\n",
      "memory usage: 10.8 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day training dataset:2012-01-29 00:00:00\n",
      "Last day training dataset:2016-04-24 00:00:00\n",
      "First day test dataset:2016-04-25 00:00:00\n",
      "Last day test dataset:2016-05-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Create a training/testing split\n",
    "\n",
    "df_train, df_test = split_train_test(data,forecast_horizon, gap)\n",
    "\n",
    "# Separate features and labels\n",
    "    \n",
    "X_train=df_train.drop(['demand'],axis=1)\n",
    "y_train=df_train['demand']\n",
    "X_test=df_test.drop(['demand'],axis=1)\n",
    "y_test=df_test['demand']\n",
    "    \n",
    "X_train.drop(columns='date',inplace=True)\n",
    "X_test.drop(columns='date',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 3 files\n",
      "Uploading ./test.csv\n",
      "Uploaded ./test.csv, 1 files out of an estimated total of 3\n",
      "Uploading ./data_walmart_tx.csv\n",
      "Uploaded ./data_walmart_tx.csv, 2 files out of an estimated total of 3\n",
      "Uploading ./train.csv\n",
      "Uploaded ./train.csv, 3 files out of an estimated total of 3\n",
      "Uploaded 3 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_8389e5f2a94347ea8d2e8bc9e5983311"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save data locally\n",
    "    \n",
    "path_data = './data_walmart_tx.csv'\n",
    "path_train = './train.csv'\n",
    "path_test = './test.csv'\n",
    "\n",
    "data.to_csv(path_data, index = None, header=True)\n",
    "df_train.to_csv(path_train, index = None, header=True)\n",
    "df_test.to_csv(path_test, index = None, header=True)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./data_walmart_tx.csv','./train.csv', './test.csv'], \n",
    "                       target_path = 'dataset/', \n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore type: AzureBlob\n",
      "Account name: mlstrg137902\n",
      "Container name: azureml-blobstore-48f83abd-b136-405f-8b52-9e940127e142\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Datastore type: \" + datastore.datastore_type,\n",
    "    \"Account name: \" + datastore.account_name,\n",
    "    \"Container name: \" + datastore.container_name,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AZUREML_DATAREFERENCE_4706f83b9cb64506b364e73aa5e0752d\n"
     ]
    }
   ],
   "source": [
    "# Get data reference object for the data path\n",
    "ds_data = datastore.path('dataset/')\n",
    "print(ds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.data.data_reference.DataReference"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_data.as_mount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "df_temp = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/train.csv'))\n",
    "df_temp = df_temp.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.data.data_reference.DataReference"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datastore.path('dataset/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>lag_revenue_t1</th>\n",
       "      <th>rolling_revenue_std_t28</th>\n",
       "      <th>rolling_revenue_mean_t28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_002_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_002</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_366</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-29</td>\n",
       "      <td>11201</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861475</td>\n",
       "      <td>0.633214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_2_007_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_007</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_366</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-29</td>\n",
       "      <td>11201</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305521</td>\n",
       "      <td>0.103929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_2_009_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_009</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_366</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-29</td>\n",
       "      <td>11201</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.394799</td>\n",
       "      <td>3.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_2_001_TX_2_evaluation</td>\n",
       "      <td>HOBBIES_2_001</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_366</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-29</td>\n",
       "      <td>11201</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.722888</td>\n",
       "      <td>0.586071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_2_002_TX_2_evaluation</td>\n",
       "      <td>HOBBIES_2_002</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_366</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-29</td>\n",
       "      <td>11201</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.759154</td>\n",
       "      <td>2.040357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_002_TX_1_evaluation  HOBBIES_2_002  HOBBIES_2  HOBBIES     TX_1   \n",
       "1  HOBBIES_2_007_TX_1_evaluation  HOBBIES_2_007  HOBBIES_2  HOBBIES     TX_1   \n",
       "2  HOBBIES_2_009_TX_1_evaluation  HOBBIES_2_009  HOBBIES_2  HOBBIES     TX_1   \n",
       "3  HOBBIES_2_001_TX_2_evaluation  HOBBIES_2_001  HOBBIES_2  HOBBIES     TX_2   \n",
       "4  HOBBIES_2_002_TX_2_evaluation  HOBBIES_2_002  HOBBIES_2  HOBBIES     TX_2   \n",
       "\n",
       "  state_id    day  demand       date  wm_yr_wk  ... day_of_week week month  \\\n",
       "0       TX  d_366       2 2012-01-29     11201  ...           6    4     1   \n",
       "1       TX  d_366       0 2012-01-29     11201  ...           6    4     1   \n",
       "2       TX  d_366       0 2012-01-29     11201  ...           6    4     1   \n",
       "3       TX  d_366       0 2012-01-29     11201  ...           6    4     1   \n",
       "4       TX  d_366       0 2012-01-29     11201  ...           6    4     1   \n",
       "\n",
       "   year  is_month_start  is_month_end  is_weekend  lag_revenue_t1  \\\n",
       "0  2012               0             0           1             0.0   \n",
       "1  2012               0             0           1             0.0   \n",
       "2  2012               0             0           1             0.0   \n",
       "3  2012               0             0           1             0.0   \n",
       "4  2012               0             0           1             0.0   \n",
       "\n",
       "   rolling_revenue_std_t28  rolling_revenue_mean_t28  \n",
       "0                 1.861475                  0.633214  \n",
       "1                 0.305521                  0.103929  \n",
       "2                 7.394799                  3.385714  \n",
       "3                 1.722888                  0.586071  \n",
       "4                 2.759154                  2.040357  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41148 entries, 0 to 41147\n",
      "Data columns (total 42 columns):\n",
      "id                          41148 non-null object\n",
      "item_id                     41148 non-null object\n",
      "dept_id                     41148 non-null object\n",
      "cat_id                      41148 non-null object\n",
      "store_id                    41148 non-null object\n",
      "state_id                    41148 non-null object\n",
      "day                         41148 non-null object\n",
      "demand                      41148 non-null int64\n",
      "date                        41148 non-null datetime64[ns]\n",
      "wm_yr_wk                    41148 non-null int64\n",
      "event_name_1                41148 non-null object\n",
      "event_type_1                41148 non-null object\n",
      "event_name_2                41148 non-null object\n",
      "event_type_2                41148 non-null object\n",
      "snap_TX                     41148 non-null int64\n",
      "sell_price                  41148 non-null float64\n",
      "lag_t28                     41148 non-null float64\n",
      "lag_t29                     41148 non-null float64\n",
      "lag_t30                     41148 non-null float64\n",
      "rolling_mean_t7             41148 non-null float64\n",
      "rolling_std_t7              41148 non-null float64\n",
      "rolling_mean_t30            41148 non-null float64\n",
      "rolling_std_t30             41148 non-null float64\n",
      "rolling_mean_t90            41148 non-null float64\n",
      "rolling_std_t90             41148 non-null float64\n",
      "rolling_mean_t180           41148 non-null float64\n",
      "rolling_std_t180            41148 non-null float64\n",
      "price_change_t1             41148 non-null float64\n",
      "price_change_t365           41148 non-null float64\n",
      "rolling_price_std_t7        41148 non-null float64\n",
      "rolling_price_std_t30       41148 non-null float64\n",
      "day_of_month                41148 non-null int64\n",
      "day_of_week                 41148 non-null int64\n",
      "week                        41148 non-null int64\n",
      "month                       41148 non-null int64\n",
      "year                        41148 non-null int64\n",
      "is_month_start              41148 non-null int64\n",
      "is_month_end                41148 non-null int64\n",
      "is_weekend                  41148 non-null int64\n",
      "lag_revenue_t1              41148 non-null float64\n",
      "rolling_revenue_std_t28     41148 non-null float64\n",
      "rolling_revenue_mean_t28    41148 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(19), int64(11), object(11)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperdrive Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters using HyperDrive\n",
    "\n",
    "The following code tune hyperparameters for the LightGBM forecast model.\n",
    "\n",
    "The ranges of parameters for the LGBM used were chosen considering the parameters tuning guides for different scenarios provided [here]( https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html).\n",
    "\n",
    "The code below does a parallel search of the hyperparameter space using a `Bayesian sampling method` which does not support `termination policy`. Therefore, `policy=None`.\n",
    "\n",
    "For Bayesian Sampling we recommend using a `maximum number of runs` greater than or equal to 20 times the number of hyperparameters being tuned. The recommendend value is 140. We set the maximum number of child runs of HyperDrive `max_total_runs` to `20` to reduce the running time. \n",
    "\n",
    "In order to compare the performance of HyperDrive with the one of AutoML we chose as [objective metric]( https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective) of LGBM `root_mean_squared_root` and we used the fact that `normalized_root_mean_squared_error` is the root_mean_squared_error divided by the range of the data. For more information check this [link]( https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#metric-normalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Estimator' is deprecated. Please use 'ScriptRunConfig' from 'azureml.core.script_run_config' with your own defined environment or an Azure ML curated environment.\n"
     ]
    }
   ],
   "source": [
    "# Increase this value if you want to achieve better performance\n",
    "max_total_runs = 20\n",
    "\n",
    "\n",
    "est = Estimator( \n",
    "    source_directory='./', # directory containing experiment configuration files (train.py)\n",
    "    compute_target=compute_target, # compute target where training will happen\n",
    "    entry_script='train.py',\n",
    "    use_docker=True,\n",
    "    script_params={\"--data-folder\": ds_data.as_mount()},\n",
    "    environment_definition=env, #remove if there is an error\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Specify hyperparameter space\n",
    "param_sampling = BayesianParameterSampling(\n",
    "    {\n",
    "        \"--num-leaves\": quniform(8, 128, 1),\n",
    "        \"--min-data-in-leaf\": quniform(20, 500, 10),\n",
    "        \"--learning-rate\": choice(\n",
    "            1e-4, 1e-3, 5e-3, 1e-2, 1.5e-2, 2e-2, 3e-2, 5e-2, 1e-1\n",
    "        ),\n",
    "        \"--feature-fraction\": uniform(0.2, 1),\n",
    "        \"--bagging-fraction\": uniform(0.1, 1),\n",
    "        \"--bagging-freq\": quniform(1, 20, 1),\n",
    "        \"--max-rounds\": quniform(50, 2000, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    estimator=est,\n",
    "    hyperparameter_sampling=param_sampling,\n",
    "    primary_metric_name='NRMSE',# normalized_root_mean_squared_error\n",
    "    primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "    max_total_runs=max_total_runs, \n",
    "    max_concurrent_runs=4,\n",
    "    policy=None, #Bayesian sampling does not support early termination policies.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n",
      "WARNING:root:If 'arguments' has been provided here and arguments have been specified in 'run_config', 'arguments' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    }
   ],
   "source": [
    "# Submit hyperdrive run to the experiment \n",
    "\n",
    "hyperdrive_run = experiment.submit(config = hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "With the help of `RunDetails` widget we can see the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672f399969af4bb491ddbeae26719071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/hyper-lgbm-walmart-forecasting/runs/HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3?wsid=/subscriptions/aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee/resourcegroups/aml-quickstarts-137902/workspaces/quick-starts-ws-137902\", \"run_id\": \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3\", \"run_properties\": {\"run_id\": \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3\", \"created_utc\": \"2021-02-07T19:27:00.765515Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"NRMSE\\\", \\\"goal\\\": \\\"minimize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"725be65c-e333-4fad-9181-54470be5ebd4\"}, \"tags\": {\"_aml_system_max_concurrent_jobs\": \"4\", \"max_concurrent_jobs\": \"4\", \"_aml_system_max_total_jobs\": \"20\", \"max_total_jobs\": \"20\", \"_aml_system_max_duration_minutes\": \"10080\", \"max_duration_minutes\": \"10080\", \"_aml_system_policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"_aml_system_generator_config\": \"{\\\"name\\\": \\\"BAYESIANOPTIMIZATION\\\", \\\"parameter_space\\\": {\\\"--num-leaves\\\": [\\\"quniform\\\", [8, 128, 1]], \\\"--min-data-in-leaf\\\": [\\\"quniform\\\", [20, 500, 10]], \\\"--learning-rate\\\": [\\\"choice\\\", [[0.0001, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1]]], \\\"--feature-fraction\\\": [\\\"uniform\\\", [0.2, 1]], \\\"--bagging-fraction\\\": [\\\"uniform\\\", [0.1, 1]], \\\"--bagging-freq\\\": [\\\"quniform\\\", [1, 20, 1]], \\\"--max-rounds\\\": [\\\"quniform\\\", [50, 2000, 10]]}}\", \"generator_config\": \"{\\\"name\\\": \\\"BAYESIANOPTIMIZATION\\\", \\\"parameter_space\\\": {\\\"--num-leaves\\\": [\\\"quniform\\\", [8, 128, 1]], \\\"--min-data-in-leaf\\\": [\\\"quniform\\\", [20, 500, 10]], \\\"--learning-rate\\\": [\\\"choice\\\", [[0.0001, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1]]], \\\"--feature-fraction\\\": [\\\"uniform\\\", [0.2, 1]], \\\"--bagging-fraction\\\": [\\\"uniform\\\", [0.1, 1]], \\\"--bagging-freq\\\": [\\\"quniform\\\", [1, 20, 1]], \\\"--max-rounds\\\": [\\\"quniform\\\", [50, 2000, 10]]}}\", \"_aml_system_primary_metric_config\": \"{\\\"name\\\": \\\"NRMSE\\\", \\\"goal\\\": \\\"minimize\\\"}\", \"primary_metric_config\": \"{\\\"name\\\": \\\"NRMSE\\\", \\\"goal\\\": \\\"minimize\\\"}\", \"_aml_system_platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://southcentralus.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee/resourceGroups/aml-quickstarts-137902/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-137902/experiments/hyper-lgbm-walmart-forecasting\\\", \\\"SubscriptionId\\\": \\\"aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\\\", \\\"ResourceGroupName\\\": \\\"aml-quickstarts-137902\\\", \\\"WorkspaceName\\\": \\\"quick-starts-ws-137902\\\", \\\"ExperimentName\\\": \\\"hyper-lgbm-walmart-forecasting\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"train.py\\\", \\\"arguments\\\": [\\\"--data-folder\\\", \\\"$AZUREML_DATAREFERENCE_4706f83b9cb64506b364e73aa5e0752d\\\"], \\\"target\\\": \\\"cpu-cluster\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": null, \\\"nodeCount\\\": 1, \\\"environment\\\": {\\\"name\\\": null, \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"azureml-defaults~=1.20.0\\\"]}, \\\"pandas\\\", \\\"numpy\\\", \\\"scipy\\\", \\\"scikit-learn\\\", \\\"lightgbm\\\", \\\"joblib\\\"], \\\"channels\\\": [\\\"anaconda\\\", \\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": true, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210104.v1\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null, \\\"registryIdentity\\\": null}, \\\"platform\\\": {\\\"os\\\": \\\"Linux\\\", \\\"architecture\\\": \\\"amd64\\\"}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": true}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"r\\\": null, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1, \\\"nodeCount\\\": 1}, \\\"paralleltask\\\": {\\\"maxRetriesPerWorker\\\": 0, \\\"workerCountPerNode\\\": 1, \\\"terminalExitCodes\\\": null}, \\\"dataReferences\\\": {\\\"4706f83b9cb64506b364e73aa5e0752d\\\": {\\\"dataStoreName\\\": \\\"workspaceblobstore\\\", \\\"pathOnDataStore\\\": \\\"dataset/\\\", \\\"mode\\\": \\\"mount\\\", \\\"overwrite\\\": false, \\\"pathOnCompute\\\": null}}, \\\"data\\\": {}, \\\"outputData\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": 1}, \\\"command\\\": \\\"\\\"}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"725be65c-e333-4fad-9181-54470be5ebd4\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"[Scrubbed]\\\", \\\"amlClientFunction\\\": \\\"[Scrubbed]\\\", \\\"tenantId\\\": \\\"660b3398-b80e-49d2-bc5b-ac1dc93b5254\\\", \\\"amlClientRequestId\\\": \\\"3439ae46-8e67-40b2-b13b-19a1b46cb193\\\", \\\"amlClientSessionId\\\": \\\"a7600eb1-ba4f-4ea0-abbd-972b4336c9c1\\\", \\\"subscriptionId\\\": \\\"aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\\\", \\\"estimator\\\": \\\"Estimator\\\", \\\"samplingMethod\\\": \\\"BayesianOptimization\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"minimize\\\", \\\"maxTotalRuns\\\": 20, \\\"maxConcurrentRuns\\\": 4, \\\"maxDurationMinutes\\\": 10080, \\\"vmSize\\\": null}}}\", \"platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://southcentralus.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee/resourceGroups/aml-quickstarts-137902/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-137902/experiments/hyper-lgbm-walmart-forecasting\\\", \\\"SubscriptionId\\\": \\\"aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\\\", \\\"ResourceGroupName\\\": \\\"aml-quickstarts-137902\\\", \\\"WorkspaceName\\\": \\\"quick-starts-ws-137902\\\", \\\"ExperimentName\\\": \\\"hyper-lgbm-walmart-forecasting\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"train.py\\\", \\\"arguments\\\": [\\\"--data-folder\\\", \\\"$AZUREML_DATAREFERENCE_4706f83b9cb64506b364e73aa5e0752d\\\"], \\\"target\\\": \\\"cpu-cluster\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": null, \\\"nodeCount\\\": 1, \\\"environment\\\": {\\\"name\\\": null, \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"azureml-defaults~=1.20.0\\\"]}, \\\"pandas\\\", \\\"numpy\\\", \\\"scipy\\\", \\\"scikit-learn\\\", \\\"lightgbm\\\", \\\"joblib\\\"], \\\"channels\\\": [\\\"anaconda\\\", \\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": true, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210104.v1\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null, \\\"registryIdentity\\\": null}, \\\"platform\\\": {\\\"os\\\": \\\"Linux\\\", \\\"architecture\\\": \\\"amd64\\\"}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": true}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"r\\\": null, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1, \\\"nodeCount\\\": 1}, \\\"paralleltask\\\": {\\\"maxRetriesPerWorker\\\": 0, \\\"workerCountPerNode\\\": 1, \\\"terminalExitCodes\\\": null}, \\\"dataReferences\\\": {\\\"4706f83b9cb64506b364e73aa5e0752d\\\": {\\\"dataStoreName\\\": \\\"workspaceblobstore\\\", \\\"pathOnDataStore\\\": \\\"dataset/\\\", \\\"mode\\\": \\\"mount\\\", \\\"overwrite\\\": false, \\\"pathOnCompute\\\": null}}, \\\"data\\\": {}, \\\"outputData\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": 1}, \\\"command\\\": \\\"\\\"}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"725be65c-e333-4fad-9181-54470be5ebd4\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"[Scrubbed]\\\", \\\"amlClientFunction\\\": \\\"[Scrubbed]\\\", \\\"tenantId\\\": \\\"660b3398-b80e-49d2-bc5b-ac1dc93b5254\\\", \\\"amlClientRequestId\\\": \\\"3439ae46-8e67-40b2-b13b-19a1b46cb193\\\", \\\"amlClientSessionId\\\": \\\"a7600eb1-ba4f-4ea0-abbd-972b4336c9c1\\\", \\\"subscriptionId\\\": \\\"aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\\\", \\\"estimator\\\": \\\"Estimator\\\", \\\"samplingMethod\\\": \\\"BayesianOptimization\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"minimize\\\", \\\"maxTotalRuns\\\": 20, \\\"maxConcurrentRuns\\\": 4, \\\"maxDurationMinutes\\\": 10080, \\\"vmSize\\\": null}}}\", \"_aml_system_resume_child_runs\": \"null\", \"resume_child_runs\": \"null\", \"_aml_system_all_jobs_generated\": \"false\", \"all_jobs_generated\": \"false\", \"_aml_system_cancellation_requested\": \"false\", \"cancellation_requested\": \"false\", \"_aml_system_progress_metadata_evaluation_timestamp\": \"\\\"2021-02-07T19:27:01.469754\\\"\", \"progress_metadata_evaluation_timestamp\": \"\\\"2021-02-07T19:27:01.469754\\\"\", \"_aml_system_progress_metadata_digest\": \"\\\"a4b073250af104d5fa9adbd55e5a43464d881f585d3a4f1d06e84b771353773f\\\"\", \"progress_metadata_digest\": \"\\\"a4b073250af104d5fa9adbd55e5a43464d881f585d3a4f1d06e84b771353773f\\\"\", \"_aml_system_progress_metadata_active_timestamp\": \"\\\"2021-02-07T19:27:01.469754\\\"\", \"progress_metadata_active_timestamp\": \"\\\"2021-02-07T19:27:01.469754\\\"\", \"_aml_system_HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_0\": \"{\\\"--num-leaves\\\": 34, \\\"--min-data-in-leaf\\\": 190, \\\"--learning-rate\\\": 0.001, \\\"--feature-fraction\\\": 0.560292589520037, \\\"--bagging-fraction\\\": 0.8517120157786802, \\\"--bagging-freq\\\": 4, \\\"--max-rounds\\\": 520}\", \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_0\": \"{\\\"--num-leaves\\\": 34, \\\"--min-data-in-leaf\\\": 190, \\\"--learning-rate\\\": 0.001, \\\"--feature-fraction\\\": 0.560292589520037, \\\"--bagging-fraction\\\": 0.8517120157786802, \\\"--bagging-freq\\\": 4, \\\"--max-rounds\\\": 520}\", \"_aml_system_HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_1\": \"{\\\"--num-leaves\\\": 121, \\\"--min-data-in-leaf\\\": 440, \\\"--learning-rate\\\": 0.001, \\\"--feature-fraction\\\": 0.4181019707267415, \\\"--bagging-fraction\\\": 0.7861083728178279, \\\"--bagging-freq\\\": 20, \\\"--max-rounds\\\": 420}\", \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_1\": \"{\\\"--num-leaves\\\": 121, \\\"--min-data-in-leaf\\\": 440, \\\"--learning-rate\\\": 0.001, \\\"--feature-fraction\\\": 0.4181019707267415, \\\"--bagging-fraction\\\": 0.7861083728178279, \\\"--bagging-freq\\\": 20, \\\"--max-rounds\\\": 420}\", \"_aml_system_HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_2\": \"{\\\"--num-leaves\\\": 29, \\\"--min-data-in-leaf\\\": 90, \\\"--learning-rate\\\": 0.01, \\\"--feature-fraction\\\": 0.6150741269574636, \\\"--bagging-fraction\\\": 0.956928012931043, \\\"--bagging-freq\\\": 10, \\\"--max-rounds\\\": 1960}\", \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_2\": \"{\\\"--num-leaves\\\": 29, \\\"--min-data-in-leaf\\\": 90, \\\"--learning-rate\\\": 0.01, \\\"--feature-fraction\\\": 0.6150741269574636, \\\"--bagging-fraction\\\": 0.956928012931043, \\\"--bagging-freq\\\": 10, \\\"--max-rounds\\\": 1960}\", \"_aml_system_HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_3\": \"{\\\"--num-leaves\\\": 96, \\\"--min-data-in-leaf\\\": 400, \\\"--learning-rate\\\": 0.02, \\\"--feature-fraction\\\": 0.23793339334409316, \\\"--bagging-fraction\\\": 0.6132134260805894, \\\"--bagging-freq\\\": 12, \\\"--max-rounds\\\": 340}\", \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_3\": \"{\\\"--num-leaves\\\": 96, \\\"--min-data-in-leaf\\\": 400, \\\"--learning-rate\\\": 0.02, \\\"--feature-fraction\\\": 0.23793339334409316, \\\"--bagging-fraction\\\": 0.6132134260805894, \\\"--bagging-freq\\\": 12, \\\"--max-rounds\\\": 340}\", \"_aml_system_environment_preparation_status\": \"PREPARING\", \"environment_preparation_status\": \"PREPARING\", \"_aml_system_prepare_run_id\": \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_preparation\", \"prepare_run_id\": \"HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3_preparation\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://mlstrg137902.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=4Eitrr3sv%2BTuvzwFsYZXagGgtXE3MSNU47HaWPg01L4%3D&st=2021-02-07T19%3A17%3A05Z&se=2021-02-08T03%3A27%3A05Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:00:48\", \"hyper_parameters\": {\"--num-leaves\": [\"quniform\", [8, 128, 1]], \"--min-data-in-leaf\": [\"quniform\", [20, 500, 10]], \"--learning-rate\": [\"choice\", [[0.0001, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1]]], \"--feature-fraction\": [\"uniform\", [0.2, 1]], \"--bagging-fraction\": [\"uniform\", [0.1, 1]], \"--bagging-freq\": [\"quniform\", [1, 20, 1]], \"--max-rounds\": [\"quniform\", [50, 2000, 10]]}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"[2021-02-07T19:27:00.990110][API][INFO]Experiment created\\r\\n[2021-02-07T19:27:01.577095][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space\\r\\n[2021-02-07T19:27:01.720804][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.\\r\\n[2021-02-07T19:27:02.0477522Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.20.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3\n",
      "Web View: https://ml.azure.com/experiments/hyper-lgbm-walmart-forecasting/runs/HD_5cdd6cc9-14ef-4995-9c00-be4d64e0cef3?wsid=/subscriptions/aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee/resourcegroups/aml-quickstarts-137902/workspaces/quick-starts-ws-137902\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "\"<START>[2021-02-07T19:27:00.990110][API][INFO]Experiment created<END>\\n\"\"<START>[2021-02-07T19:27:01.577095][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space<END>\\n\"\"<START>[2021-02-07T19:27:01.720804][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.<END>\\n\"<START>[2021-02-07T19:27:02.0477522Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.<END>\n"
     ]
    }
   ],
   "source": [
    "# Show run details with the Jupyter widget\n",
    "RunDetails(hyperdrive_run).show()\n",
    "hyperdrive_run.wait_for_completion(show_output=True)\n",
    "hyperdrive_run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Save Best Model\n",
    "\n",
    "Here we retrieve and save the best model as well as display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model and its hyperparameter values\n",
    "\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()[\"runDefinition\"][\"arguments\"]\n",
    "\n",
    "\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('NRMSE:', best_run_metrics['NRMSE'])\n",
    "print('Best model hyperparameter values', parameter_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model = best_run.register_model(\n",
    "    model_name=\"hd_lgbm_walmart_forecast\", \n",
    "    model_path=\"./outputs/bst-model.pkl\",\n",
    "    description='Best HyperDrive Walmart forecasting model'\n",
    ")\n",
    "print(\"Model successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "### Create score script \n",
    "\n",
    "The scoring script created for deploying the best model obtained here was called `score_hd.py` and can be find [here](https://github.com/dpbac/Forecasting-Walmart-sales-with-Azure/blob/master/score_hd.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create myenv.yml\n",
    "\n",
    "We also need to create an environment file so that Azure Machine Learning can install the necessary packages in the Docker image which are required by your scoring script. In this case, we need to specify packages `numpy`, `pandas`, and `lightgbm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package(\"numpy=1.18.5\")\n",
    "cd.add_conda_package(\"pandas=0.25.3\")\n",
    "cd.add_conda_package(\"lightgbm=2.3.0\")\n",
    "cd.save_to_file(base_directory=\"./\", conda_file_path=\"myenv.yml\")\n",
    "\n",
    "print(cd.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI\n",
    "\n",
    "We are almost ready to deploy. In the next cell, we first create the inference configuration and deployment configuration. Then, we deploy the model to ACI. This cell will run for several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_config = InferenceConfig(runtime=\"python\", entry_script=\"score_hd.py\", conda_file=\"myenv.yml\")\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 2, \n",
    "                                               auth_enabled=True, \n",
    "                                               enable_app_insights=True,\n",
    "                                               tags = {'type': \"hd-lgbm-forecasting\"},\n",
    "                                               description = \"LightGBM model on Walmart Texas stores data\")\n",
    "\n",
    "\n",
    "aci_service_name = 'hd-walmart-forecast'\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=aci_service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment state: \" + service.state)\n",
    "print(\"Scoring URI: \" + service.scoring_uri)\n",
    "print(\"Authetication Key: \" + service.get_keys()[0])\n",
    "print(\"Swagger URI: \" + service.swagger_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model\n",
    "\n",
    "Let's test the deployed model. We create a few test data points and send them to the web service hosted in ACI. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions each of which represents the forecasted sales of a target store, brand in a given week as specified by `store, brand, week` in `used_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test features (28 days)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28 days in the test features dataset\n",
    "X_test['day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.reset_index(drop=True, inplace = True)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(drop=True, inplace = True)\n",
    "\n",
    "import json\n",
    "X_query = X_test.iloc[:3]\n",
    "\n",
    "# The Service object accept the complex dictionary, which is internally converted to JSON string.\n",
    "# The section 'data' contains the data frame in the form of dictionary.\n",
    "test_sample = json.dumps({'data': X_query.to_dict(orient='records')})\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pick a few test data points\n",
    "# test_samples = json.dumps({\"data\": np.array(X_test.iloc[:3]).tolist()})\n",
    "# test_samples = bytes(test_samples, encoding=\"utf8\")\n",
    "# test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "print(\"prediction:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send raw HTTP request to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, test_samples, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "print(\"\")\n",
    "print(\"input data:\", test_samples)\n",
    "print(\"\")\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Service\n",
    "\n",
    "After finishing the tests, you can delete the ACI deployment with a simple delete API call as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
